{
  "experiment_name": "Gradient Boosting with Feature Engineering",
  "task_type": "binary_classification",
  "target_column": "y",
  "experiment_evaluation_metric": "ROC-AUC",
  "experiment_preprocessing_steps": "1. Load train/test datasets with column mapping: V1=age, V2=job, V3=marital, V4=education, V5=default, V6=balance, V7=housing, V8=loan, V9=contact, V10=day, V11=month, V12=duration, V13=campaign, V14=pdays, V15=previous, V16=poutcome, target=y. 2. Convert target from (1,2) to (0,1) by subtracting 1. 3. Apply mixed encoding strategy: one-hot encode 'job' and 'month' columns (high cardinality), label encode remaining categorical columns ['marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']. 4. No outlier clipping or scaling needed for tree-based models.",
  "experiment_feature_engineering_steps": "1. Duration-based features: duration_log = log1p(duration), duration_short = (duration < 100), duration_long = (duration > 500). 2. Balance-based features: balance_log = log(balance) where balance > 0 else 0, balance_negative = (balance < 0), balance_zero = (balance == 0). 3. Campaign features: campaign_multiple = (campaign > 1), pdays_contacted_before = (pdays != -1). 4. Seasonal features: month_high_success = month in ['mar', 'dec', 'sep'], month_low_success = month in ['may', 'jul', 'jun']. 5. Previous outcome features: poutcome_success = (poutcome == 'success'), poutcome_unknown = (poutcome == 'unknown'). Final feature count: 48 features after encoding.",
  "experiment_model_selection_steps": "1. Use Gradient Boosting Classifier as primary model (achieved highest ROC-AUC of 0.9335 in experiments). 2. Hyperparameters: n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42. 3. Implement 5-fold stratified cross-validation for model evaluation to handle class imbalance. 4. Compare against Random Forest and Extra Trees as baseline comparisons. 5. Train final model on full training set and evaluate on holdout test set.",
  "experiment_evaluation_strategy": "1. Primary metric: ROC-AUC (optimized for imbalanced binary classification). 2. Cross-validation: 5-fold stratified CV to maintain class distribution. 3. Additional evaluation analyses: a) Feature importance ranking to identify top predictors, b) Probability calibration analysis using calibration plots, c) Performance by class: precision, recall, F1-score for both classes, d) Performance by key segments: analyze ROC-AUC by job type, age groups, and campaign characteristics, e) Prediction confidence distribution analysis, f) ROC curve and precision-recall curve visualization. 4. Error analysis: identify misclassified samples patterns, analyze duration thresholds impact, examine seasonal performance variations. 5. Model diagnostics: learning curves, validation curves for key hyperparameters.",
  "expected_outputs": "1. Trained GradientBoostingClassifier model saved as .pkl file. 2. ROC-AUC score on test set (expected >0.93 based on CV). 3. Feature importance DataFrame with rankings. 4. Classification report with precision/recall/F1 for both classes. 5. Calibration plot and ROC/PR curves. 6. Performance analysis by segments (job, age, month). 7. Model evaluation report with all metrics and diagnostics. 8. Predictions CSV file with probabilities for test set."
}